{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2BviHfGO7qEitnL6tr5oK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TroTusk/Contacts-classification/blob/main/contacts_classifcation_structural_bioinfo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB install and imports\n",
        "Sets up XGBoost, mounts Drive, and imports all libraries (NumPy/Pandas, scikit-learn, XGBoost)."
      ],
      "metadata": {
        "id": "4uF4jYUEorb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnNVUpF6AhdM",
        "outputId": "eb6c453b-b08a-4a9a-deef-cf7ba9c3bd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip -q install xgboost\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, glob, time, json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, matthews_corrcoef,\n",
        "    classification_report, confusion_matrix, average_precision_score, roc_auc_score\n",
        ")\n",
        "from xgboost import XGBClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paths, options, and helper functions\n",
        "Defines project paths and small helpers:\n",
        "\n",
        "\n",
        "*   load_from_features_ring to read multiple TSVs\n",
        "*   ap_auc_macro to compute macro AP/AUC\n",
        "*   subsample_binary to build balanced one-vs-rest subsets quickly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SXe3cQ-to95b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites (read before running)\n",
        "\n",
        "1) Download or clone the project locally, then **place the whole project folder in your Google Drive** at:\n",
        "\n",
        "/content/drive/MyDrive/progetto_bioinfo_finale/project\n",
        "\n",
        "If you prefer a different location, you must update `DRIVE_PROJECT_DIR` in the next cell.\n"
      ],
      "metadata": {
        "id": "5PyvyGRZxLP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Project root on Drive\n",
        "DRIVE_PROJECT_DIR = \"/content/drive/MyDrive/progetto_bioinfo_finale/project\" # edit if you prefer a different location\n",
        "FEATURES_DIR = os.path.join(DRIVE_PROJECT_DIR, \"features_ring\")\n",
        "FALLBACK_DATASET = os.path.join(DRIVE_PROJECT_DIR, \"complete_training_dataset.tsv\")\n",
        "\n",
        "# output names\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PROJECT_DIR, f\"contact_models_{ts}.joblib\")\n",
        "\n",
        "# options\n",
        "MAX_FILES   = 1000  # reduce for lighter training\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# print directories\n",
        "print(\"Project:\", DRIVE_PROJECT_DIR)\n",
        "print(\"features_ring exists:\", os.path.isdir(FEATURES_DIR))\n",
        "print(\"fallback dataset exists:\", os.path.exists(FALLBACK_DATASET))\n",
        "\n",
        "# set rng\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "load up to `max_files` TSVs from `root`, add a `__pdb__` column from the filename,\n",
        "concatenate them, and return a single DataFrame (or None if nothing loads).\n",
        "\"\"\"\n",
        "def load_from_features_ring(root, max_files=MAX_FILES):\n",
        "\n",
        "    files = sorted(glob.glob(os.path.join(root, \"*.tsv\")))\n",
        "    if not files:\n",
        "        return None\n",
        "    if max_files and len(files) > max_files:\n",
        "        files = list(rng.choice(files, size=max_files, replace=False))\n",
        "    frames = []\n",
        "    for fp in files:\n",
        "        try:\n",
        "            df = pd.read_csv(fp, sep=\"\\t\")\n",
        "            df[\"__pdb__\"] = Path(fp).stem\n",
        "            frames.append(df)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not frames:\n",
        "        return None\n",
        "    return pd.concat(frames, ignore_index=True)\n",
        "\n",
        "# compute macro Average Precision and macro ROC AUC in one-vs-rest. returns (ap_macro, auc_macro)\n",
        "def ap_auc_macro(y_true: pd.Series, Proba: pd.DataFrame, classes: list):\n",
        "    Yb = pd.get_dummies(pd.Categorical(y_true, categories=classes)).reindex(columns=classes, fill_value=0).to_numpy()\n",
        "    A  = Proba.loc[y_true.index, classes].to_numpy()\n",
        "    aps, aucs = [], []\n",
        "    for i in range(len(classes)):\n",
        "        pos = Yb[:, i].sum()\n",
        "        neg = len(Yb) - pos\n",
        "        if pos > 0:\n",
        "            aps.append(average_precision_score(Yb[:, i], A[:, i]))\n",
        "        if pos > 0 and neg > 0:\n",
        "            aucs.append(roc_auc_score(Yb[:, i], A[:, i]))\n",
        "    apm  = float(np.mean(aps))  if aps  else np.nan\n",
        "    aucm = float(np.mean(aucs)) if aucs else np.nan\n",
        "    return apm, aucm\n",
        "\n",
        "# build a one-vs-rest training subset\n",
        "def subsample_binary(y_series, pos_label, neg_pos_ratio=3, max_pos=None, rnd=rng):\n",
        "    idx_pos = np.where(y_series.values == pos_label)[0]\n",
        "    idx_neg = np.where(y_series.values != pos_label)[0]\n",
        "    if max_pos is not None and len(idx_pos) > max_pos:\n",
        "        idx_pos = rnd.choice(idx_pos, size=max_pos, replace=False)\n",
        "    n_neg = min(len(idx_neg), int(len(idx_pos) * neg_pos_ratio))\n",
        "    if n_neg < len(idx_neg):\n",
        "        idx_neg = rnd.choice(idx_neg, size=n_neg, replace=False)\n",
        "    idx = np.concatenate([idx_pos, idx_neg])\n",
        "    rnd.shuffle(idx)\n",
        "    return idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS339Fi-BH0X",
        "outputId": "10841ad8-053c-43d1-9818-8919bfa09fea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project: /content/drive/MyDrive/progetto_bioinfo_finale/project\n",
            "features_ring exists: True\n",
            "fallback dataset exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset (features_ring first, fallback otherwise)\n",
        "Loads per-PDB TSVs from features_ring/ or falls back to the merged TSV. Drops unlabeled rows and “Unclassified”. Prints basic dataset stats."
      ],
      "metadata": {
        "id": "1i7HWlLtBLDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try features_ring\n",
        "df_all = load_from_features_ring(FEATURES_DIR, max_files=MAX_FILES)"
      ],
      "metadata": {
        "id": "laE-7syAIcaJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# otherwise use the fallback dataset (training_dataset_completo.tsv present on the project folder)\n",
        "if df_all is None and os.path.exists(FALLBACK_DATASET):\n",
        "    df_all = pd.read_csv(FALLBACK_DATASET, sep=\"\\t\")\n",
        "    print(\"using fallback dataset\")\n",
        "    if \"__pdb__\" not in df_all.columns and \"pdb_id\" in df_all.columns:\n",
        "        df_all[\"__pdb__\"] = df_all[\"pdb_id\"].astype(str)"
      ],
      "metadata": {
        "id": "M5mwSnoBIxnU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "assert df_all is not None, \"No data found. Ensure features_ring/ or training_dataset_completo.tsv is available.\"\n",
        "\n",
        "# remove 'Unclassified' rows\n",
        "df_all = df_all.dropna(subset=[\"Interaction\"]).copy()\n",
        "df_all = df_all[df_all[\"Interaction\"].astype(str) != \"Unclassified\"].copy()\n",
        "\n",
        "print(\"Rows:\", len(df_all))\n",
        "print(\"Columns:\", len(df_all.columns))\n",
        "print(\"Classes:\", sorted(df_all[\"Interaction\"].astype(str).unique().tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML0hhXozBLfz",
        "outputId": "dcd68bb3-fdef-4f01-baa7-700c64a97e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 479608\n",
            "Columns: 33\n",
            "Classes: ['HBOND', 'IONIC', 'PICATION', 'PIHBOND', 'PIPISTACK', 'SSBOND', 'VDW']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build features\n",
        "Creates the feature matrix: numeric columns with mean imputation and one-hot encoding for categorical variables (optionally adds 3Di letters if present)."
      ],
      "metadata": {
        "id": "fvAqfnlMBOL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric columns (spec)\n",
        "num_cols = [\n",
        "    \"s_rsa\",\"s_phi\",\"s_psi\",\"s_a1\",\"s_a2\",\"s_a3\",\"s_a4\",\"s_a5\",\n",
        "    \"t_rsa\",\"t_phi\",\"t_psi\",\"t_a1\",\"t_a2\",\"t_a3\",\"t_a4\",\"t_a5\",\n",
        "]\n",
        "# keep only the numeric columns that actually exist in this dataset\n",
        "num_cols = [c for c in num_cols if c in df_all.columns]\n",
        "\n",
        "# categorical feature columns\n",
        "cat_cols = [\"s_resn\",\"t_resn\",\"s_ss8\",\"t_ss8\"]\n",
        "\n",
        "# include 3Di letters if present in the files\n",
        "for c in [\"s_3di_letter\",\"t_3di_letter\"]:\n",
        "    if c in df_all.columns:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "# Build feature matrix\n",
        "X_base = df_all[num_cols + cat_cols].copy()\n",
        "\n",
        "# numeric imputation, cast to numeric and fill any missing values with the column mean\n",
        "for c in num_cols:\n",
        "    X_base[c] = pd.to_numeric(X_base[c]).fillna(X_base[c].mean())\n",
        "\n",
        "\n",
        "# One-hot encode only the categorical columns that are present\n",
        "present_cats = [c for c in cat_cols if c in X_base.columns]\n",
        "X = pd.get_dummies(X_base, columns=present_cats, dummy_na=False)\n",
        "\n",
        "\n",
        "# Targets (interaction labels) and grouping key (per-PDB) for group-wise splitting\n",
        "y = df_all[\"Interaction\"].astype(str).copy()\n",
        "groups = df_all[\"__pdb__\"].astype(str) if \"__pdb__\" in df_all.columns else pd.Series([\"_all_\"]*len(df_all))\n",
        "\n",
        "print(\"Final #features:\", X.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAlewMOcBN7r",
        "outputId": "10ed1139-d3ef-4ef2-c7e5-c2977e39ae22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final #features: 114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split by PDB\n",
        "Performs a train/validation split using PDB IDs so the same structure doesn’t leak across splits. Prints class distribution."
      ],
      "metadata": {
        "id": "fBDuh9GeBVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# splitting training and validation\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=RANDOM_SEED)\n",
        "tr_idx, va_idx = next(gss.split(X, y, groups=groups))\n",
        "\n",
        "# slice features and labels\n",
        "X_tr, X_va = X.iloc[tr_idx].copy(), X.iloc[va_idx].copy()\n",
        "y_tr, y_va = y.iloc[tr_idx].copy(), y.iloc[va_idx].copy()\n",
        "\n",
        "# class list and the exact feature set used by the model\n",
        "classes = sorted(y.unique().tolist())\n",
        "model_columns = X_tr.columns.tolist()\n",
        "\n",
        "print(\"Train rows:\", len(X_tr), \"Validation rows:\", len(X_va))\n",
        "print(\"Classes:\", classes)\n",
        "print(\"\\nTraining distribution:\")\n",
        "print(y_tr.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRtZsBcwBU0W",
        "outputId": "c9e0992f-048b-4c0b-9814-01e9ad2475e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 386784 Validation rows: 92824\n",
            "Classes: ['HBOND', 'IONIC', 'PICATION', 'PIHBOND', 'PIPISTACK', 'SSBOND', 'VDW']\n",
            "\n",
            "Training distribution:\n",
            "Interaction\n",
            "HBOND        218523\n",
            "VDW          150475\n",
            "PIPISTACK      7723\n",
            "IONIC          7456\n",
            "PICATION       1779\n",
            "SSBOND          467\n",
            "PIHBOND         361\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train OVR with ExtraTrees (baseline)\n",
        "Trains one ExtraTrees binary classifier per class on subsampled data (balanced positives/negatives)."
      ],
      "metadata": {
        "id": "hUdL8OyrBZbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train one binary model per class (one-vs-rest) with light subsampling\n",
        "models = {}\n",
        "for i, c in enumerate(classes, 1):\n",
        "    # pick all positives for class c and a limited number of negatives\n",
        "    idx = subsample_binary(y_tr, c, neg_pos_ratio=3, max_pos=20000)\n",
        "    Xt, yt = X_tr.iloc[idx], (y_tr.iloc[idx] == c).astype(int)\n",
        "\n",
        "    # extra-trees\n",
        "    clf = ExtraTreesClassifier(\n",
        "        n_estimators=250,          # number of trees\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=3,        # small leaves to reduce overfitting\n",
        "        max_features=\"sqrt\",\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        n_jobs=-1,                 # use all cores\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    # fit the binary classifier for class c\n",
        "    clf.fit(Xt, yt)\n",
        "\n",
        "    # store the model for this class\n",
        "    models[c] = clf\n",
        "\n",
        "    # progress print\n",
        "    print(f\"[ET  {i}/{len(classes)}] trained {c:9s} on {len(idx)} rows ({int(yt.sum())} positives)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_ZHA9ejBZwy",
        "outputId": "f324b19b-5b9a-4627-ced7-5ed6192909d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ET  1/7] trained HBOND     on 80000 rows (20000 positives)\n",
            "[ET  2/7] trained IONIC     on 29824 rows (7456 positives)\n",
            "[ET  3/7] trained PICATION  on 7116 rows (1779 positives)\n",
            "[ET  4/7] trained PIHBOND   on 1444 rows (361 positives)\n",
            "[ET  5/7] trained PIPISTACK on 30892 rows (7723 positives)\n",
            "[ET  6/7] trained SSBOND    on 1868 rows (467 positives)\n",
            "[ET  7/7] trained VDW       on 80000 rows (20000 positives)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model (opzional) uncomment the next rows to allow saving\n",
        "# joblib.dump({\"models\": models, \"columns\": model_columns, \"classes\": classes}, MODEL_PATH)\n",
        "# print(\"Saved:\", MODEL_PATH)"
      ],
      "metadata": {
        "id": "RuZNdOogJrnj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate ExtraTrees\n",
        "Computes probabilities, predicts by argmax, and prints Accuracy, Balanced Accuracy, MCC, macro AP and AUC, plus a classification report and confusion matrix."
      ],
      "metadata": {
        "id": "VVRRk0QGBcVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_va_et = np.column_stack([models[c].predict_proba(X_va)[:, 1] for c in classes]).astype(np.float32)\n",
        "P_va_et = pd.DataFrame(P_va_et, columns=classes, index=X_va.index)\n",
        "y_pred_et = P_va_et.idxmax(axis=1)\n",
        "# calculates all the metrics\n",
        "acc  = accuracy_score(y_va, y_pred_et)\n",
        "ba   = balanced_accuracy_score(y_va, y_pred_et)\n",
        "mcc  = matthews_corrcoef(y_va, y_pred_et)\n",
        "apm, aucm = ap_auc_macro(y_va, P_va_et, classes)\n",
        "\n",
        "print(\"Validation (ExtraTrees, argmax)\")\n",
        "print(\"Accuracy:\", f\"{acc:.4f}\")\n",
        "print(\"Balanced Accuracy:\", f\"{ba:.4f}\")\n",
        "print(\"MCC:\", f\"{mcc:.4f}\")\n",
        "print(\"Average Precision (macro):\", \"n/a\" if np.isnan(apm) else f\"{apm:.4f}\")\n",
        "print(\"ROC AUC (macro):\", \"n/a\" if np.isnan(aucm) else f\"{aucm:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_va, y_pred_et, zero_division=0))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_va, y_pred_et))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwnSalI5BcIS",
        "outputId": "d099113e-af92-46d3-aa74-1e3a1580dced"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation (ExtraTrees, argmax)\n",
            "Accuracy: 0.5001\n",
            "Balanced Accuracy: 0.7618\n",
            "MCC: 0.1915\n",
            "Average Precision (macro): 0.3789\n",
            "ROC AUC (macro): 0.8707\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HBOND       0.66      0.54      0.59     52476\n",
            "       IONIC       0.18      0.99      0.31      1756\n",
            "    PICATION       0.13      0.98      0.22       419\n",
            "     PIHBOND       0.01      0.43      0.02        92\n",
            "   PIPISTACK       0.46      1.00      0.63      1802\n",
            "      SSBOND       0.41      1.00      0.59        98\n",
            "         VDW       0.49      0.39      0.44     36181\n",
            "\n",
            "    accuracy                           0.50     92824\n",
            "   macro avg       0.33      0.76      0.40     92824\n",
            "weighted avg       0.57      0.50      0.52     92824\n",
            "\n",
            "Confusion matrix:\n",
            "[[28081  4978  1463  2059   931    47 14917]\n",
            " [    2  1747     0     0     0     0     7]\n",
            " [    0     0   409     8     0     0     2]\n",
            " [    7     0    17    40    19     0     9]\n",
            " [    0     0     0     3  1798     0     1]\n",
            " [    0     0     0     0     0    98     0]\n",
            " [14664  2811  1336  1835  1198    92 14245]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train OVR with XGBoost\n",
        "Trains one XGBoost binary classifier per class with class imbalance handled via scale_pos_weight. Uses the same subsampling recipe. Saving is left commented."
      ],
      "metadata": {
        "id": "YVI_xHh1Bgoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train one binary XGBoost model per class (one-vs-rest) with light subsampling\n",
        "xgb_models = {}\n",
        "for i, c in enumerate(classes, 1):\n",
        "    # pick all positives for class c and a limited number of negatives\n",
        "    idx = subsample_binary(y_tr, c, neg_pos_ratio=3, max_pos=20000)\n",
        "    Xt, yt = X_tr.iloc[idx], (y_tr.iloc[idx] == c).astype(int)\n",
        "\n",
        "    # compute class imbalance for this subset used by scale_pos_weight\n",
        "    pos = int(yt.sum()); neg = len(yt) - pos\n",
        "    spw = (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "    # xgboost setup to generalizes well\n",
        "    clf = XGBClassifier(\n",
        "        n_estimators=400,          # number of boosting rounds\n",
        "        max_depth=6,               # tree depth (bias/variance knob)\n",
        "        learning_rate=0.10,        # step size per round\n",
        "        subsample=0.85,            # row sampling for regularization\n",
        "        colsample_bytree=0.70,     # feature sampling per tree\n",
        "        reg_lambda=1.0,            # L2 regularization\n",
        "        objective=\"binary:logistic\",\n",
        "        scale_pos_weight=spw,      # fix class imbalance within this subset\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    # fit the binary classifier for class c\n",
        "    clf.fit(Xt, yt)\n",
        "\n",
        "    # store the model for this class\n",
        "    xgb_models[c] = clf\n",
        "\n",
        "    # progress log\n",
        "    print(f\"[XGB {i}/{len(classes)}] {c:9s} — {len(idx)} rows ({pos} pos)\")\n",
        "\n",
        "\n",
        "# Saving the model (opzional) uncomment the next rows to allow saving\n",
        "# XGB_MODEL_PATH = os.path.join(DRIVE_PROJECT_DIR, f\"xgb_models_{ts}.joblib\")\n",
        "# joblib.dump({\"models\": xgb_models, \"columns\": model_columns, \"classes\": classes}, XGB_MODEL_PATH)\n",
        "# print(\"Saved:\", XGB_MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQZZuouiBgb9",
        "outputId": "931f31c9-4da2-47cc-9b77-b927faa9bfc3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB 1/7] HBOND     — 80000 rows (20000 pos)\n",
            "[XGB 2/7] IONIC     — 29824 rows (7456 pos)\n",
            "[XGB 3/7] PICATION  — 7116 rows (1779 pos)\n",
            "[XGB 4/7] PIHBOND   — 1444 rows (361 pos)\n",
            "[XGB 5/7] PIPISTACK — 30892 rows (7723 pos)\n",
            "[XGB 6/7] SSBOND    — 1868 rows (467 pos)\n",
            "[XGB 7/7] VDW       — 80000 rows (20000 pos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate XGBoost\n",
        "Same evaluation as cell 7 but for the XGBoost ensemble, so you can compare ExtraTrees vs XGBoost on the same validation split."
      ],
      "metadata": {
        "id": "SqlNeyssBjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_va_xgb = np.column_stack([xgb_models[c].predict_proba(X_va)[:,1] for c in classes]).astype(np.float32)\n",
        "P_va_xgb = pd.DataFrame(P_va_xgb, columns=classes, index=X_va.index)\n",
        "y_pred_xgb = P_va_xgb.idxmax(axis=1)\n",
        "# calculates the metrics\n",
        "acc  = accuracy_score(y_va, y_pred_xgb)\n",
        "ba   = balanced_accuracy_score(y_va, y_pred_xgb)\n",
        "mcc  = matthews_corrcoef(y_va, y_pred_xgb)\n",
        "apm, aucm = ap_auc_macro(y_va, P_va_xgb, classes)\n",
        "\n",
        "print(\"Validation (XGBoost, argmax)\")\n",
        "print(\"Accuracy:\", f\"{acc:.4f}\")\n",
        "print(\"Balanced Accuracy:\", f\"{ba:.4f}\")\n",
        "print(\"MCC:\", f\"{mcc:.4f}\")\n",
        "print(\"Average Precision (macro):\", \"n/a\" if np.isnan(apm) else f\"{apm:.4f}\")\n",
        "print(\"ROC AUC (macro):\", \"n/a\" if np.isnan(aucm) else f\"{aucm:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_va, y_pred_xgb, zero_division=0))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_va, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a560eEzsBjHL",
        "outputId": "af947615-543c-4e19-ac1e-13d8d0a516ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation (XGBoost, argmax)\n",
            "Accuracy: 0.5121\n",
            "Balanced Accuracy: 0.7321\n",
            "MCC: 0.2025\n",
            "Average Precision (macro): 0.3630\n",
            "ROC AUC (macro): 0.8708\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HBOND       0.66      0.55      0.60     52476\n",
            "       IONIC       0.19      0.99      0.32      1756\n",
            "    PICATION       0.13      0.88      0.22       419\n",
            "     PIHBOND       0.01      0.38      0.02        92\n",
            "   PIPISTACK       0.45      0.91      0.60      1802\n",
            "      SSBOND       0.41      1.00      0.58        98\n",
            "         VDW       0.50      0.41      0.45     36181\n",
            "\n",
            "    accuracy                           0.51     92824\n",
            "   macro avg       0.34      0.73      0.40     92824\n",
            "weighted avg       0.58      0.51      0.53     92824\n",
            "\n",
            "Confusion matrix:\n",
            "[[28642  4656  1338  1793   865    47 15135]\n",
            " [   10  1742     0     0     0     0     4]\n",
            " [    1     0   370    45     0     0     3]\n",
            " [   10     0    16    35    17     0    14]\n",
            " [    0     0     0   164  1637     0     1]\n",
            " [    0     0     0     0     0    98     0]\n",
            " [14466  2667  1217  1635  1092    93 15011]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test XGB on a random PDB\n",
        "Builds features for a single PDB TSV, runs the XGBoost ensemble, and reports the metrics. Shows a small table of mismatches."
      ],
      "metadata": {
        "id": "i3hQzzrvMOwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test on a random TSV from features_ring\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, matthews_corrcoef,\n",
        "    classification_report, confusion_matrix, average_precision_score, roc_auc_score\n",
        ")\n",
        "\n",
        "# check if the trained model exists\n",
        "if len(xgb_models) > 0:\n",
        "    ensemble = xgb_models\n",
        "    model_name = \"XGBoost (OVR)\"\n",
        "else:\n",
        "    raise RuntimeError(\"No trained ensemble found. Run the training cells first.\")\n",
        "\n",
        "\n",
        "\n",
        "# pick a random TSV\n",
        "tsv_files = sorted(glob.glob(os.path.join(FEATURES_DIR, \"*.tsv\")))\n",
        "\n",
        "test_path = np.random.choice(tsv_files)\n",
        "df_test = pd.read_csv(test_path, sep=\"\\t\")\n",
        "name = Path(test_path).stem\n",
        "\n",
        "# build features exactly like training\n",
        "NUM_ALL = [\n",
        "    \"s_rsa\",\"s_phi\",\"s_psi\",\"s_a1\",\"s_a2\",\"s_a3\",\"s_a4\",\"s_a5\",\n",
        "    \"t_rsa\",\"t_phi\",\"t_psi\",\"t_a1\",\"t_a2\",\"t_a3\",\"t_a4\",\"t_a5\",\n",
        "]\n",
        "CAT_ALL = [\"s_resn\",\"t_resn\",\"s_ss8\",\"t_ss8\"]\n",
        "\n",
        "for c in [\"s_3di_letter\",\"t_3di_letter\"]:\n",
        "    if c in df_test.columns:\n",
        "        CAT_ALL.append(c)\n",
        "\n",
        "num_cols_test = [c for c in NUM_ALL if c in df_test.columns]\n",
        "cat_cols_test = [c for c in CAT_ALL if c in df_test.columns]\n",
        "\n",
        "Xt = df_test[num_cols_test + cat_cols_test].copy()\n",
        "for c in num_cols_test:\n",
        "    Xt[c] = pd.to_numeric(Xt[c], errors=\"coerce\").fillna(Xt[c].mean())\n",
        "Xt = pd.get_dummies(Xt, columns=cat_cols_test, dummy_na=False)\n",
        "\n",
        "# align to training columns\n",
        "Xt = Xt.reindex(columns=model_columns, fill_value=0).astype(np.float32)\n",
        "\n",
        "# probabilities and predictions\n",
        "P = np.column_stack([ensemble[c].predict_proba(Xt)[:, 1] for c in classes]).astype(np.float32)\n",
        "P = pd.DataFrame(P, columns=classes, index=Xt.index)\n",
        "yhat = P.idxmax(axis=1)\n",
        "\n",
        "print(f\"Random test file: {name}\")\n",
        "print(f\"model: {model_name} \\n\")\n",
        "\n",
        "# evaluate if labels exist\n",
        "\n",
        "mask = df_test[\"Interaction\"].notna()\n",
        "y_true = df_test.loc[mask, \"Interaction\"].astype(str)\n",
        "y_pred = yhat.loc[mask]\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "ba  = balanced_accuracy_score(y_true, y_pred)\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", f\"{acc:.4f}\")\n",
        "print(\"Balanced Accuracy:\", f\"{ba:.4f}\")\n",
        "print(\"MCC:\", f\"{mcc:.4f}\")\n",
        "print(\"Average Precision (macro):\", \"n/a\" if np.isnan(apm) else f\"{apm:.4f}\")\n",
        "print(\"ROC AUC (macro):\", \"n/a\" if np.isnan(aucm) else f\"{aucm:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHPNuPydwIa6",
        "outputId": "375d2229-2ded-49a5-8062-e207bacfd19a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random test file: 6fbz\n",
            "model: XGBoost (OVR) \n",
            "\n",
            "Accuracy: 0.5517\n",
            "Balanced Accuracy: 0.7771\n",
            "MCC: 0.3009\n",
            "Average Precision (macro): 0.3630\n",
            "ROC AUC (macro): 0.8708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
          ]
        }
      ]
    }
  ]
}